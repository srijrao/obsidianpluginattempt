/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var __publicField = (obj, key, value) => __defNormalProp(obj, typeof key !== "symbol" ? key + "" : key, value);

// main.ts
var main_exports = {};
__export(main_exports, {
  default: () => MyPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian = require("obsidian");

// types.ts
var DEFAULT_SETTINGS = {
  provider: "openai",
  openaiSettings: {
    apiKey: "",
    model: "gpt-4",
    availableModels: []
  },
  anthropicSettings: {
    apiKey: "",
    model: "claude-3-sonnet-20240229",
    availableModels: []
  },
  geminiSettings: {
    apiKey: "",
    model: "gemini-pro",
    availableModels: []
  },
  ollamaSettings: {
    serverUrl: "http://localhost:11434",
    model: "llama2",
    availableModels: []
  },
  systemMessage: "You are a helpful assistant.",
  temperature: 0.7,
  maxTokens: 1e3,
  includeDateWithSystemMessage: false,
  includeTimeWithSystemMessage: false,
  enableStreaming: true,
  autoOpenModelSettings: true,
  enableObsidianLinks: true
};

// providers/base.ts
var ProviderError = class extends Error {
  constructor(type, message, statusCode) {
    super(message);
    __publicField(this, "type");
    __publicField(this, "statusCode");
    this.type = type;
    this.statusCode = statusCode;
    this.name = "ProviderError";
  }
};
var BaseProvider = class {
  /**
   * Handle common HTTP errors
   */
  handleHttpError(error) {
    if (error instanceof Response) {
      const status2 = error.status;
      switch (status2) {
        case 401:
          throw new ProviderError(
            "invalid_api_key" /* InvalidApiKey */,
            "Invalid API key",
            status2
          );
        case 429:
          throw new ProviderError(
            "rate_limit" /* RateLimit */,
            "Rate limit exceeded",
            status2
          );
        case 400:
          throw new ProviderError(
            "invalid_request" /* InvalidRequest */,
            "Invalid request",
            status2
          );
        case 500:
        case 502:
        case 503:
        case 504:
          throw new ProviderError(
            "server_error" /* ServerError */,
            "Server error occurred",
            status2
          );
        default:
          throw new ProviderError(
            "server_error" /* ServerError */,
            `Unknown error occurred: ${status2}`,
            status2
          );
      }
    }
    if (!error.response) {
      throw new ProviderError(
        "network_error" /* NetworkError */,
        "Network error occurred"
      );
    }
    const status = error.response.status;
    switch (status) {
      case 401:
        throw new ProviderError(
          "invalid_api_key" /* InvalidApiKey */,
          "Invalid API key",
          status
        );
      case 429:
        throw new ProviderError(
          "rate_limit" /* RateLimit */,
          "Rate limit exceeded",
          status
        );
      case 400:
        throw new ProviderError(
          "invalid_request" /* InvalidRequest */,
          "Invalid request",
          status
        );
      case 500:
      case 502:
      case 503:
      case 504:
        throw new ProviderError(
          "server_error" /* ServerError */,
          "Server error occurred",
          status
        );
      default:
        throw new ProviderError(
          "server_error" /* ServerError */,
          `Unknown error occurred: ${status}`,
          status
        );
    }
  }
  /**
   * Format error message for connection test results
   */
  formatErrorMessage(error) {
    if (error instanceof ProviderError) {
      switch (error.type) {
        case "invalid_api_key" /* InvalidApiKey */:
          return "Invalid API key. Please check your credentials.";
        case "rate_limit" /* RateLimit */:
          return "Rate limit exceeded. Please try again later.";
        case "network_error" /* NetworkError */:
          return "Network error. Please check your internet connection.";
        default:
          return error.message;
      }
    }
    return error.message || "An unknown error occurred";
  }
  /**
   * Create a standard error response for connection tests
   */
  createErrorResponse(error) {
    return {
      success: false,
      message: this.formatErrorMessage(error)
    };
  }
};

// providers/anthropic.ts
var AnthropicProvider = class extends BaseProvider {
  constructor(apiKey, model = "claude-3-sonnet-20240229") {
    super();
    __publicField(this, "apiKey");
    __publicField(this, "baseUrl", "https://api.anthropic.com/v1");
    __publicField(this, "model");
    this.apiKey = apiKey;
    this.model = model;
  }
  /**
   * Get a completion from Anthropic
   * 
   * Sends the conversation to Anthropic and streams back the response.
   * 
   * @param messages - The conversation history
   * @param options - Settings for this completion
   */
  async getCompletion(messages, options) {
    var _a, _b, _c, _d, _e;
    try {
      const response = await fetch(`${this.baseUrl}/messages`, {
        method: "POST",
        headers: {
          "anthropic-api-key": this.apiKey,
          "anthropic-version": "2024-02-15",
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: this.model,
          messages: messages.map((msg) => ({
            role: msg.role,
            content: msg.content
          })),
          temperature: (_a = options.temperature) != null ? _a : 0.7,
          max_tokens: (_b = options.maxTokens) != null ? _b : 1e3,
          stream: true
        }),
        signal: (_c = options.abortController) == null ? void 0 : _c.signal
      });
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const reader = (_d = response.body) == null ? void 0 : _d.getReader();
      const decoder = new TextDecoder("utf-8");
      let buffer = "";
      while (true) {
        const { done, value } = await (reader == null ? void 0 : reader.read()) || { done: true, value: void 0 };
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.startsWith("data: ") && line !== "data: [DONE]") {
            try {
              const data = JSON.parse(line.slice(6));
              const content = (_e = data.content[0]) == null ? void 0 : _e.text;
              if (content && options.streamCallback) {
                options.streamCallback(content);
              }
            } catch (e) {
              console.warn("Error parsing Anthropic response chunk:", e);
            }
          }
        }
      }
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      if (error.name === "AbortError") {
        console.log("Anthropic stream was aborted");
      } else {
        console.error("Error calling Anthropic:", error);
        throw error;
      }
    }
  }
  /**
   * Get available Anthropic models
   * 
   * Returns the list of supported Claude models.
   * Note: Anthropic doesn't have a models endpoint, so we return known models.
   * 
   * @returns List of available model names
   */
  async getAvailableModels() {
    return [
      "claude-3-opus-20240229",
      "claude-3-sonnet-20240229",
      "claude-3-haiku-20240307"
    ];
  }
  /**
   * Test connection to Anthropic
   * 
   * Verifies the API key works by attempting a simple completion.
   * 
   * @returns Test results including success/failure
   */
  async testConnection() {
    try {
      const response = await fetch(`${this.baseUrl}/messages`, {
        method: "POST",
        headers: {
          "anthropic-api-key": this.apiKey,
          "anthropic-version": "2024-02-15",
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: this.model,
          messages: [{ role: "user", content: "Hi" }],
          max_tokens: 1
        })
      });
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const models = await this.getAvailableModels();
      return {
        success: true,
        message: "Successfully connected to Anthropic!",
        models
      };
    } catch (error) {
      return this.createErrorResponse(error);
    }
  }
};

// providers/openai.ts
var OpenAIProvider = class extends BaseProvider {
  constructor(apiKey, model = "gpt-4") {
    super();
    __publicField(this, "apiKey");
    __publicField(this, "baseUrl", "https://api.openai.com/v1");
    __publicField(this, "model");
    this.apiKey = apiKey;
    this.model = model;
  }
  /**
   * Get a completion from OpenAI
   * 
   * Sends the conversation to OpenAI and streams back the response.
   * 
   * @param messages - The conversation history
   * @param options - Settings for this completion
   */
  async getCompletion(messages, options) {
    var _a, _b, _c, _d, _e, _f;
    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: "POST",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: (_a = options.temperature) != null ? _a : 0.7,
          max_tokens: (_b = options.maxTokens) != null ? _b : 1e3,
          stream: true
        }),
        signal: (_c = options.abortController) == null ? void 0 : _c.signal
      });
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const reader = (_d = response.body) == null ? void 0 : _d.getReader();
      const decoder = new TextDecoder("utf-8");
      let buffer = "";
      while (true) {
        const { done, value } = await (reader == null ? void 0 : reader.read()) || { done: true, value: void 0 };
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.startsWith("data: ") && line !== "data: [DONE]") {
            try {
              const data = JSON.parse(line.slice(6));
              const content = (_f = (_e = data.choices[0]) == null ? void 0 : _e.delta) == null ? void 0 : _f.content;
              if (content && options.streamCallback) {
                options.streamCallback(content);
              }
            } catch (e) {
              console.warn("Error parsing OpenAI response chunk:", e);
            }
          }
        }
      }
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      if (error.name === "AbortError") {
        console.log("OpenAI stream was aborted");
      } else {
        console.error("Error calling OpenAI:", error);
        throw error;
      }
    }
  }
  /**
   * Get available OpenAI models
   * 
   * Fetches the list of models from OpenAI's API.
   * Filters to only include chat models (GPT-3.5, GPT-4, etc.)
   * 
   * @returns List of available model names
   */
  async getAvailableModels() {
    try {
      const response = await fetch(`${this.baseUrl}/models`, {
        method: "GET",
        headers: {
          "Authorization": `Bearer ${this.apiKey}`,
          "Content-Type": "application/json"
        }
      });
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const data = await response.json();
      return data.data.map((model) => model.id).filter((id) => id.startsWith("gpt-"));
    } catch (error) {
      console.error("Error fetching OpenAI models:", error);
      throw error;
    }
  }
  /**
   * Test connection to OpenAI
   * 
   * Verifies the API key works by attempting to list models.
   * 
   * @returns Test results including success/failure and available models
   */
  async testConnection() {
    try {
      const models = await this.getAvailableModels();
      return {
        success: true,
        message: `Successfully connected to OpenAI! Found ${models.length} available models.`,
        models
      };
    } catch (error) {
      return this.createErrorResponse(error);
    }
  }
};

// providers/gemini.ts
var GeminiProvider = class extends BaseProvider {
  constructor(apiKey, model = "gemini-pro") {
    super();
    __publicField(this, "apiKey");
    __publicField(this, "baseUrl", "https://generativelanguage.googleapis.com/v1");
    __publicField(this, "model");
    this.apiKey = apiKey;
    this.model = model;
  }
  /**
   * Convert messages to Gemini format
   * 
   * @param messages - Standard message format
   * @returns Messages in Gemini format
   */
  convertToGeminiFormat(messages) {
    return messages.map((msg) => ({
      role: msg.role === "assistant" ? "model" : msg.role,
      parts: [{ text: msg.content }]
    }));
  }
  /**
   * Get a completion from Gemini
   * 
   * Sends the conversation to Gemini and streams back the response.
   * 
   * @param messages - The conversation history
   * @param options - Settings for this completion
   */
  async getCompletion(messages, options) {
    var _a, _b, _c, _d, _e, _f, _g, _h, _i;
    try {
      const contents = this.convertToGeminiFormat(messages);
      const response = await fetch(
        `${this.baseUrl}/models/${this.model}:streamGenerateContent?key=${this.apiKey}`,
        {
          method: "POST",
          headers: {
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            contents,
            generationConfig: {
              temperature: (_a = options.temperature) != null ? _a : 0.7,
              maxOutputTokens: (_b = options.maxTokens) != null ? _b : 1e3,
              topP: 0.8,
              topK: 40
            }
          }),
          signal: (_c = options.abortController) == null ? void 0 : _c.signal
        }
      );
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const reader = (_d = response.body) == null ? void 0 : _d.getReader();
      const decoder = new TextDecoder("utf-8");
      let buffer = "";
      while (true) {
        const { done, value } = await (reader == null ? void 0 : reader.read()) || { done: true, value: void 0 };
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.trim()) {
            try {
              const data = JSON.parse(line);
              const content = (_i = (_h = (_g = (_f = (_e = data.candidates) == null ? void 0 : _e[0]) == null ? void 0 : _f.content) == null ? void 0 : _g.parts) == null ? void 0 : _h[0]) == null ? void 0 : _i.text;
              if (content && options.streamCallback) {
                options.streamCallback(content);
              }
            } catch (e) {
              console.warn("Error parsing Gemini response chunk:", e);
            }
          }
        }
      }
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      if (error.name === "AbortError") {
        console.log("Gemini stream was aborted");
      } else {
        console.error("Error calling Gemini:", error);
        throw error;
      }
    }
  }
  /**
   * Get available Gemini models
   * 
   * Fetches the list of models from Gemini's API.
   * 
   * @returns List of available model names
   */
  async getAvailableModels() {
    try {
      const response = await fetch(
        `${this.baseUrl}/models?key=${this.apiKey}`
      );
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const data = await response.json();
      return data.models.map((model) => model.name).filter((name) => name.includes("gemini"));
    } catch (error) {
      console.error("Error fetching Gemini models:", error);
      return ["gemini-pro", "gemini-pro-vision"];
    }
  }
  /**
   * Test connection to Gemini
   * 
   * Verifies the API key works by attempting to list models.
   * 
   * @returns Test results including success/failure
   */
  async testConnection() {
    try {
      const response = await fetch(
        `${this.baseUrl}/models?key=${this.apiKey}`
      );
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const models = await this.getAvailableModels();
      return {
        success: true,
        message: "Successfully connected to Google Gemini!",
        models
      };
    } catch (error) {
      return this.createErrorResponse(error);
    }
  }
};

// providers/ollama.ts
var OllamaProvider = class extends BaseProvider {
  constructor(serverUrl = "http://localhost:11434", model = "llama2") {
    super();
    __publicField(this, "apiKey", "");
    // Not used for Ollama
    __publicField(this, "baseUrl");
    __publicField(this, "model");
    this.baseUrl = serverUrl.replace(/\/$/, "");
    this.model = model;
  }
  /**
   * Convert messages to Ollama format
   * 
   * @param messages - Standard message format
   * @returns Prompt string in Ollama format
   */
  convertToOllamaFormat(messages) {
    return messages.map((msg) => {
      if (msg.role === "system") {
        return `System: ${msg.content}

`;
      }
      return `${msg.role === "user" ? "Human" : "Assistant"}: ${msg.content}

`;
    }).join("") + "Assistant:";
  }
  /**
   * Get a completion from Ollama
   * 
   * Sends the conversation to the local Ollama server and streams back the response.
   * 
   * @param messages - The conversation history
   * @param options - Settings for this completion
   */
  async getCompletion(messages, options) {
    var _a, _b, _c, _d;
    try {
      const prompt = this.convertToOllamaFormat(messages);
      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json"
        },
        body: JSON.stringify({
          model: this.model,
          prompt,
          stream: true,
          options: {
            temperature: (_a = options.temperature) != null ? _a : 0.7,
            num_predict: (_b = options.maxTokens) != null ? _b : 1e3
          }
        }),
        signal: (_c = options.abortController) == null ? void 0 : _c.signal
      });
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const reader = (_d = response.body) == null ? void 0 : _d.getReader();
      const decoder = new TextDecoder("utf-8");
      let buffer = "";
      while (true) {
        const { done, value } = await (reader == null ? void 0 : reader.read()) || { done: true, value: void 0 };
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.trim()) {
            try {
              const data = JSON.parse(line);
              if (data.response && options.streamCallback) {
                options.streamCallback(data.response);
              }
            } catch (e) {
              console.warn("Error parsing Ollama response chunk:", e);
            }
          }
        }
      }
    } catch (error) {
      if (error instanceof ProviderError) {
        throw error;
      }
      if (error.name === "AbortError") {
        console.log("Ollama stream was aborted");
      } else {
        console.error("Error calling Ollama:", error);
        throw error;
      }
    }
  }
  /**
   * Get available Ollama models
   * 
   * Fetches the list of models installed on the local Ollama server.
   * 
   * @returns List of available model names
   */
  async getAvailableModels() {
    var _a;
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      if (!response.ok) {
        throw this.handleHttpError(response);
      }
      const data = await response.json();
      return ((_a = data.models) == null ? void 0 : _a.map((model) => model.name)) || [];
    } catch (error) {
      console.error("Error fetching Ollama models:", error);
      throw error;
    }
  }
  /**
   * Test connection to Ollama
   * 
   * Verifies the Ollama server is running and accessible.
   * Also checks if any models are installed.
   * 
   * @returns Test results including success/failure and available models
   */
  async testConnection() {
    try {
      const models = await this.getAvailableModels();
      if (models.length === 0) {
        return {
          success: false,
          message: 'Connected to Ollama server, but no models are installed. Use "ollama pull model-name" to install models.',
          models: []
        };
      }
      return {
        success: true,
        message: `Successfully connected to Ollama! Found ${models.length} installed models.`,
        models
      };
    } catch (error) {
      return this.createErrorResponse(error);
    }
  }
};

// providers/index.ts
function createProvider(settings) {
  switch (settings.provider) {
    case "openai":
      return new OpenAIProvider(
        settings.openaiSettings.apiKey,
        settings.openaiSettings.model
      );
    case "anthropic":
      return new AnthropicProvider(
        settings.anthropicSettings.apiKey,
        settings.anthropicSettings.model
      );
    case "gemini":
      return new GeminiProvider(
        settings.geminiSettings.apiKey,
        settings.geminiSettings.model
      );
    case "ollama":
      return new OllamaProvider(
        settings.ollamaSettings.serverUrl,
        settings.ollamaSettings.model
      );
    default:
      throw new Error(`Invalid provider type: ${settings.provider}`);
  }
}

// main.ts
var VIEW_TYPE_MODEL_SETTINGS = "model-settings-view";
var ModelSettingsView = class extends import_obsidian.ItemView {
  constructor(leaf, plugin) {
    super(leaf);
    __publicField(this, "plugin");
    this.plugin = plugin;
  }
  getViewType() {
    return VIEW_TYPE_MODEL_SETTINGS;
  }
  getDisplayText() {
    return "AI Model Settings";
  }
  async onOpen() {
    const { contentEl } = this;
    contentEl.empty();
    contentEl.createEl("h2", { text: "AI Model Settings" });
    contentEl.createEl("h3", { text: "Common Settings" });
    new import_obsidian.Setting(contentEl).setName("AI Provider").setDesc("Choose which AI provider to use").addDropdown((dropdown) => {
      dropdown.addOption("openai", "OpenAI (GPT-3.5, GPT-4)").addOption("anthropic", "Anthropic (Claude)").addOption("gemini", "Google (Gemini)").addOption("ollama", "Ollama (Local AI)").setValue(this.plugin.settings.provider).onChange(async (value) => {
        this.plugin.settings.provider = value;
        await this.plugin.saveSettings();
        this.onOpen();
      });
    });
    new import_obsidian.Setting(contentEl).setName("System Message").setDesc("Set the system message for the AI").addTextArea((text) => text.setPlaceholder("You are a helpful assistant.").setValue(this.plugin.settings.systemMessage).onChange(async (value) => {
      this.plugin.settings.systemMessage = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Include Date with System Message").setDesc("Add the current date to the system message").addToggle((toggle) => toggle.setValue(this.plugin.settings.includeDateWithSystemMessage).onChange(async (value) => {
      this.plugin.settings.includeDateWithSystemMessage = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Include Time with System Message").setDesc("Add the current time along with the date to the system message").addToggle((toggle) => toggle.setValue(this.plugin.settings.includeTimeWithSystemMessage).onChange(async (value) => {
      this.plugin.settings.includeTimeWithSystemMessage = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Enable Obsidian Links").setDesc("Read Obsidian links in messages using [[filename]] syntax").addToggle((toggle) => toggle.setValue(this.plugin.settings.enableObsidianLinks).onChange(async (value) => {
      this.plugin.settings.enableObsidianLinks = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Enable Streaming").setDesc("Enable or disable streaming for completions").addToggle((toggle) => toggle.setValue(this.plugin.settings.enableStreaming).onChange(async (value) => {
      this.plugin.settings.enableStreaming = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Temperature").setDesc("Set the randomness of the model's output (0-1)").addSlider((slider) => slider.setLimits(0, 1, 0.1).setValue(this.plugin.settings.temperature).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.temperature = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(contentEl).setName("Max Tokens").setDesc("Set the maximum length of the model's output").addText((text) => text.setPlaceholder("4000").setValue(String(this.plugin.settings.maxTokens)).onChange(async (value) => {
      const numValue = Number(value);
      if (!isNaN(numValue)) {
        this.plugin.settings.maxTokens = numValue;
        await this.plugin.saveSettings();
      }
    }));
    contentEl.createEl("h3", { text: `${this.plugin.settings.provider.toUpperCase()} Settings` });
    switch (this.plugin.settings.provider) {
      case "openai":
        this.renderOpenAISettings(contentEl);
        break;
      case "anthropic":
        this.renderAnthropicSettings(contentEl);
        break;
      case "gemini":
        this.renderGeminiSettings(contentEl);
        break;
      case "ollama":
        this.renderOllamaSettings(contentEl);
        break;
    }
  }
  renderOpenAISettings(containerEl) {
    const settings = this.plugin.settings.openaiSettings;
    new import_obsidian.Setting(containerEl).setName("Test Connection").setDesc("Verify your API key and fetch available models").addButton((button) => button.setButtonText("Test").onClick(async () => {
      button.setButtonText("Testing...");
      button.setDisabled(true);
      try {
        const provider = createProvider(this.plugin.settings);
        const result = await provider.testConnection();
        if (result.success && result.models) {
          settings.availableModels = result.models;
          await this.plugin.saveSettings();
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: true,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
          this.onOpen();
        } else {
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: false,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
        }
      } catch (error) {
        new import_obsidian.Notice(`Error: ${error.message}`);
      } finally {
        button.setButtonText("Test");
        button.setDisabled(false);
      }
    }));
    if (settings.lastTestResult) {
      const date = new Date(settings.lastTestResult.timestamp);
      containerEl.createEl("div", {
        text: `Last test: ${date.toLocaleString()} - ${settings.lastTestResult.message}`,
        cls: settings.lastTestResult.success ? "success" : "error"
      });
    }
    new import_obsidian.Setting(containerEl).setName("Model").setDesc("Choose the OpenAI model to use").addDropdown((dropdown) => {
      for (const model of settings.availableModels) {
        dropdown.addOption(model, model);
      }
      dropdown.setValue(settings.model).onChange(async (value) => {
        settings.model = value;
        await this.plugin.saveSettings();
      });
    });
  }
  renderAnthropicSettings(containerEl) {
    const settings = this.plugin.settings.anthropicSettings;
    new import_obsidian.Setting(containerEl).setName("Test Connection").setDesc("Verify your API key and fetch available models").addButton((button) => button.setButtonText("Test").onClick(async () => {
      button.setButtonText("Testing...");
      button.setDisabled(true);
      try {
        const provider = createProvider(this.plugin.settings);
        const result = await provider.testConnection();
        if (result.success && result.models) {
          settings.availableModels = result.models;
          await this.plugin.saveSettings();
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: true,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
          this.onOpen();
        } else {
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: false,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
        }
      } catch (error) {
        new import_obsidian.Notice(`Error: ${error.message}`);
      } finally {
        button.setButtonText("Test");
        button.setDisabled(false);
      }
    }));
    if (settings.lastTestResult) {
      const date = new Date(settings.lastTestResult.timestamp);
      containerEl.createEl("div", {
        text: `Last test: ${date.toLocaleString()} - ${settings.lastTestResult.message}`,
        cls: settings.lastTestResult.success ? "success" : "error"
      });
    }
    new import_obsidian.Setting(containerEl).setName("Model").setDesc("Choose the Anthropic model to use").addDropdown((dropdown) => {
      for (const model of settings.availableModels) {
        dropdown.addOption(model, model);
      }
      dropdown.setValue(settings.model).onChange(async (value) => {
        settings.model = value;
        await this.plugin.saveSettings();
      });
    });
  }
  renderGeminiSettings(containerEl) {
    const settings = this.plugin.settings.geminiSettings;
    new import_obsidian.Setting(containerEl).setName("Test Connection").setDesc("Verify your API key and fetch available models").addButton((button) => button.setButtonText("Test").onClick(async () => {
      button.setButtonText("Testing...");
      button.setDisabled(true);
      try {
        const provider = createProvider(this.plugin.settings);
        const result = await provider.testConnection();
        if (result.success && result.models) {
          settings.availableModels = result.models;
          await this.plugin.saveSettings();
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: true,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
          this.onOpen();
        } else {
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: false,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
        }
      } catch (error) {
        new import_obsidian.Notice(`Error: ${error.message}`);
      } finally {
        button.setButtonText("Test");
        button.setDisabled(false);
      }
    }));
    if (settings.lastTestResult) {
      const date = new Date(settings.lastTestResult.timestamp);
      containerEl.createEl("div", {
        text: `Last test: ${date.toLocaleString()} - ${settings.lastTestResult.message}`,
        cls: settings.lastTestResult.success ? "success" : "error"
      });
    }
    new import_obsidian.Setting(containerEl).setName("Model").setDesc("Choose the Gemini model to use").addDropdown((dropdown) => {
      for (const model of settings.availableModels) {
        dropdown.addOption(model, model);
      }
      dropdown.setValue(settings.model).onChange(async (value) => {
        settings.model = value;
        await this.plugin.saveSettings();
      });
    });
  }
  renderOllamaSettings(containerEl) {
    const settings = this.plugin.settings.ollamaSettings;
    new import_obsidian.Setting(containerEl).setName("Test Connection").setDesc("Check server connection and fetch available models").addButton((button) => button.setButtonText("Test").onClick(async () => {
      button.setButtonText("Testing...");
      button.setDisabled(true);
      try {
        const provider = createProvider(this.plugin.settings);
        const result = await provider.testConnection();
        if (result.success && result.models) {
          settings.availableModels = result.models;
          await this.plugin.saveSettings();
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: true,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
          this.onOpen();
        } else {
          settings.lastTestResult = {
            timestamp: Date.now(),
            success: false,
            message: result.message
          };
          new import_obsidian.Notice(result.message);
        }
      } catch (error) {
        new import_obsidian.Notice(`Error: ${error.message}`);
      } finally {
        button.setButtonText("Test");
        button.setDisabled(false);
      }
    }));
    if (settings.lastTestResult) {
      const date = new Date(settings.lastTestResult.timestamp);
      containerEl.createEl("div", {
        text: `Last test: ${date.toLocaleString()} - ${settings.lastTestResult.message}`,
        cls: settings.lastTestResult.success ? "success" : "error"
      });
    }
    new import_obsidian.Setting(containerEl).setName("Model").setDesc("Choose the Ollama model to use").addDropdown((dropdown) => {
      for (const model of settings.availableModels) {
        dropdown.addOption(model, model);
      }
      dropdown.setValue(settings.model).onChange(async (value) => {
        settings.model = value;
        await this.plugin.saveSettings();
      });
    });
    containerEl.createEl("div", {
      cls: "setting-item-description",
      text: "To use Ollama:"
    });
    const steps = containerEl.createEl("ol");
    steps.createEl("li", { text: "Install Ollama from https://ollama.ai" });
    steps.createEl("li", { text: "Start the Ollama server" });
    steps.createEl("li", { text: 'Pull models using "ollama pull model-name"' });
    steps.createEl("li", { text: "Test connection to see available models" });
  }
  async onClose() {
  }
};
function parseSelection(selection) {
  const lines = selection.split("\n");
  let messages = [];
  let currentRole = "user";
  let currentContent = "";
  for (const line of lines) {
    if (line.trim() === "----") {
      if (currentContent.trim()) {
        messages.push({ role: currentRole, content: currentContent.trim() });
      }
      currentRole = currentRole === "user" ? "assistant" : "user";
      currentContent = "";
    } else {
      currentContent += line + "\n";
    }
  }
  if (currentContent.trim()) {
    messages.push({ role: currentRole, content: currentContent.trim() });
  }
  return messages;
}
var MyPlugin = class extends import_obsidian.Plugin {
  constructor() {
    super(...arguments);
    __publicField(this, "settings");
    __publicField(this, "modelSettingsView", null);
    __publicField(this, "activeStream", null);
  }
  async onload() {
    await this.loadSettings();
    this.addSettingTab(new MyPluginSettingTab(this.app, this));
    this.registerView(
      VIEW_TYPE_MODEL_SETTINGS,
      (leaf) => new ModelSettingsView(leaf, this)
    );
    this.addRibbonIcon("gear", "Open AI Settings", () => {
      this.activateView();
    });
    this.app.workspace.onLayoutReady(() => {
      if (this.settings.autoOpenModelSettings) {
        this.activateView();
      }
    });
    this.addCommand({
      id: "ai-completion",
      name: "Get AI Completion",
      editorCallback: async (editor) => {
        let text;
        let insertPosition;
        if (editor.somethingSelected()) {
          text = editor.getSelection();
          insertPosition = editor.getCursor("to");
        } else {
          const lineNumber = editor.getCursor().line;
          const documentText = editor.getValue();
          const lines = documentText.split("\n").slice(0, lineNumber + 1);
          text = lines.join("\n");
          insertPosition = { line: lineNumber + 1, ch: 0 };
        }
        const messages = parseSelection(text);
        editor.replaceRange("\n\n----\n\n", insertPosition);
        let currentPosition = {
          line: insertPosition.line + 3,
          ch: 0
        };
        this.activeStream = new AbortController();
        try {
          const provider = createProvider(this.settings);
          const processedMessages = await this.processMessages([
            { role: "system", content: this.getSystemMessage() },
            ...messages
          ]);
          await provider.getCompletion(
            processedMessages,
            {
              temperature: this.settings.temperature,
              maxTokens: this.settings.maxTokens,
              streamCallback: (chunk) => {
                editor.replaceRange(chunk, currentPosition);
                currentPosition = editor.offsetToPos(
                  editor.posToOffset(currentPosition) + chunk.length
                );
              },
              abortController: this.activeStream
            }
          );
          editor.replaceRange("\n\n----\n\n", currentPosition);
          const newCursorPos = editor.offsetToPos(
            editor.posToOffset(currentPosition) + 8
          );
          editor.setCursor(newCursorPos);
        } catch (error) {
          new import_obsidian.Notice(`Error: ${error.message}`);
          editor.replaceRange(`Error: ${error.message}

----

`, currentPosition);
        } finally {
          this.activeStream = null;
        }
      }
    });
    this.addCommand({
      id: "end-ai-stream",
      name: "End AI Stream",
      callback: () => {
        if (this.activeStream) {
          this.activeStream.abort();
          this.activeStream = null;
          new import_obsidian.Notice("AI stream ended");
        } else {
          new import_obsidian.Notice("No active AI stream to end");
        }
      }
    });
    this.addCommand({
      id: "show-ai-settings",
      name: "Show AI Settings",
      callback: () => {
        this.activateView();
      }
    });
  }
  getSystemMessage() {
    let systemMessage = this.settings.systemMessage;
    if (this.settings.includeDateWithSystemMessage) {
      const currentDate = (/* @__PURE__ */ new Date()).toISOString().split("T")[0];
      if (this.settings.includeTimeWithSystemMessage) {
        const currentTime = (/* @__PURE__ */ new Date()).toLocaleTimeString();
        systemMessage = `${systemMessage} The current date and time is ${currentDate} ${currentTime}.`;
      } else {
        systemMessage = `${systemMessage} The current date is ${currentDate}.`;
      }
    }
    return systemMessage;
  }
  async activateView() {
    this.app.workspace.detachLeavesOfType(VIEW_TYPE_MODEL_SETTINGS);
    let leaf = this.app.workspace.getRightLeaf(false);
    if (leaf) {
      await leaf.setViewState({
        type: VIEW_TYPE_MODEL_SETTINGS,
        active: true
      });
      this.app.workspace.revealLeaf(leaf);
    } else {
      leaf = this.app.workspace.getLeaf(true);
      await leaf.setViewState({
        type: VIEW_TYPE_MODEL_SETTINGS,
        active: true
      });
    }
  }
  async loadSettings() {
    this.settings = Object.assign({}, DEFAULT_SETTINGS, await this.loadData());
  }
  async saveSettings() {
    await this.saveData(this.settings);
  }
  /**
   * Processes a message content to include Obsidian note contents
   * 
   * If a link is found, retrieves the note content and appends it after the link.
   * 
   * @param content The message content to process
   * @returns The processed content with note contents included
   */
  /**
   * Process an array of messages to include Obsidian note contents
   * 
   * @param messages Array of messages to process
   * @returns Promise resolving to processed messages
   */
  async processMessages(messages) {
    const processedMessages = [];
    for (const message of messages) {
      const processedContent = await this.processObsidianLinks(message.content);
      processedMessages.push({
        role: message.role,
        content: processedContent
      });
    }
    return processedMessages;
  }
  /**
   * Process a single message content to include Obsidian note contents
   * 
   * @param content The message content to process
   * @returns Promise resolving to processed content
   */
  async processObsidianLinks(content) {
    if (!this.settings.enableObsidianLinks) return content;
    const linkRegex = /\[\[(.*?)\]\]/g;
    let match;
    let processedContent = content;
    while ((match = linkRegex.exec(content)) !== null) {
      const fileName = match[1];
      try {
        const file = this.app.vault.getAbstractFileByPath(`${fileName}.md`);
        if (file && file instanceof import_obsidian.TFile) {
          const noteContent = await this.app.vault.cachedRead(file);
          processedContent = processedContent.replace(
            match[0],
            `${match[0]}
${fileName}:
${noteContent}
`
          );
        }
      } catch (error) {
        console.error(`Error processing Obsidian link for ${fileName}:`, error);
      }
    }
    return processedContent;
  }
};
var MyPluginSettingTab = class extends import_obsidian.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    __publicField(this, "plugin");
    this.plugin = plugin;
  }
  /**
   * Display the settings tab
   * 
   * Shows only the auto-open setting here since all other settings
   * are managed in the model settings view for better organization.
   */
  display() {
    const { containerEl } = this;
    containerEl.empty();
    containerEl.createEl("h2", { text: "AI Assistant Settings" });
    containerEl.createEl("h3", { text: "API Keys" });
    new import_obsidian.Setting(containerEl).setName("OpenAI API Key").setDesc("Enter your OpenAI API key").addText((text) => text.setPlaceholder("Enter your API key").setValue(this.plugin.settings.openaiSettings.apiKey).onChange(async (value) => {
      this.plugin.settings.openaiSettings.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(containerEl).setName("Anthropic API Key").setDesc("Enter your Anthropic API key").addText((text) => text.setPlaceholder("Enter your API key").setValue(this.plugin.settings.anthropicSettings.apiKey).onChange(async (value) => {
      this.plugin.settings.anthropicSettings.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(containerEl).setName("Google API Key").setDesc("Enter your Google API key").addText((text) => text.setPlaceholder("Enter your API key").setValue(this.plugin.settings.geminiSettings.apiKey).onChange(async (value) => {
      this.plugin.settings.geminiSettings.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(containerEl).setName("Ollama Server URL").setDesc("Enter your Ollama server URL (default: http://localhost:11434)").addText((text) => text.setPlaceholder("http://localhost:11434").setValue(this.plugin.settings.ollamaSettings.serverUrl).onChange(async (value) => {
      this.plugin.settings.ollamaSettings.serverUrl = value;
      await this.plugin.saveSettings();
    }));
    containerEl.createEl("h3", { text: "Model Settings" });
    new import_obsidian.Setting(containerEl).setName("Auto-open Model Settings").setDesc("Automatically open model settings when Obsidian starts").addToggle((toggle) => toggle.setValue(this.plugin.settings.autoOpenModelSettings).onChange(async (value) => {
      this.plugin.settings.autoOpenModelSettings = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(containerEl).setName("Open Model Settings").setDesc("Open the model settings view").addButton((button) => button.setButtonText("Open").onClick(() => {
      this.plugin.activateView();
    }));
  }
};
